{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b2b789a",
   "metadata": {},
   "source": [
    "# CNN(Fashion MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68ffdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b34e3628",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = train_data,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    drop_last = True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset = test_data,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fbae19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(  \n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=32*6*6, out_features=64, bias = True)\n",
    "        self.drop = nn.Dropout2d(0.5)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=32, bias = True)\n",
    "        self.fc3 = nn.Linear(in_features=32, out_features=10, bias = True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e74d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "\n",
    "# loss and optimizer\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f54be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Print(nn.Module):\n",
    "    def forward(self, x):\n",
    "        print(x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c590f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train(model, criterion, optimizer, data):\n",
    "    model.train() # training\n",
    "    for X,y in data:\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred,y)\n",
    "        \n",
    "        optimizer.zero_grad() # zero_grad(): 미분값 초기화\n",
    "        loss.backward() # backpropagation\n",
    "        optimizer.step() # update weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e1d2266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, criterion, optimizer, data):\n",
    "    model.eval() #evaluation\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    with torch.no_grad(): # no update\n",
    "        for X, y in data:\n",
    "            y_pred = model(X)\n",
    "            test_loss += criterion(y_pred, y)\n",
    "            \n",
    "            prediction = y_pred.max(1)[1] # max indices\n",
    "            corrects = (prediction == y)\n",
    "            accuracy += corrects.sum().float() / float( y.size(0) )\n",
    "    return test_loss, accuracy        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c24be2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loss= 36.90299987792969 accu= 86.74002075195312\n",
      "2 loss= 31.972232818603516 accu= 88.40000915527344\n",
      "3 loss= 30.5328426361084 accu= 89.19000244140625\n",
      "4 loss= 28.015920639038086 accu= 90.0400161743164\n",
      "5 loss= 28.3795166015625 accu= 89.83999633789062\n",
      "6 loss= 27.672943115234375 accu= 89.66000366210938\n",
      "7 loss= 27.51436996459961 accu= 90.1600112915039\n",
      "8 loss= 26.226613998413086 accu= 90.31001281738281\n",
      "9 loss= 26.224544525146484 accu= 90.56998443603516\n",
      "10 loss= 24.85728645324707 accu= 90.87999725341797\n",
      "11 loss= 25.225576400756836 accu= 90.82999420166016\n",
      "12 loss= 29.968059539794922 accu= 88.69001007080078\n",
      "13 loss= 25.253923416137695 accu= 91.24999237060547\n",
      "14 loss= 24.24906349182129 accu= 91.23001098632812\n",
      "15 loss= 24.295604705810547 accu= 91.31998443603516\n",
      "16 loss= 25.83555793762207 accu= 91.01000213623047\n",
      "17 loss= 25.035242080688477 accu= 91.30000305175781\n",
      "18 loss= 24.250638961791992 accu= 91.70999908447266\n",
      "19 loss= 25.44548797607422 accu= 90.90995788574219\n",
      "20 loss= 24.43114471435547 accu= 91.47998809814453\n"
     ]
    }
   ],
   "source": [
    "epoch = 20\n",
    "for epoch in range(epoch):\n",
    "    train(model, criterion, optimizer, train_loader)\n",
    "    \n",
    "    E, accu = evaluation(model, criterion, optimizer, test_loader)\n",
    "    print(epoch+1, \"loss=\", E.item(), \"accu=\", accu.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba87844",
   "metadata": {},
   "source": [
    "가끔 local minima에 빠지는 모습을 볼 수 있고, overfitting으로 의심되는 점(혹은 미니배치에 의한 oscillation)으로 볼 수 있는 것은 15번째 이후이다. 15번 정도에서 학습을 멈추면 될 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad6275",
   "metadata": {},
   "source": [
    "## 소감문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5059c4ca",
   "metadata": {},
   "source": [
    "글을 읽으면서 과학 철학에 도구론적 반사실론자 중 데이비드 머민의 “shut up and calculate!”라는 말이 떠올랐습니다. 양자역학을 이해하려는 사람에게 한 유명한 말인데, 이 글이 말하고자 하는 내용과 같다고 생각했습니다. 과학은 이 세계를 이해하기 위한 도구이고, 진리라고 생각해서는 안된다는 말이었습니다. 단백질 접힘 구조를 설명하려는 머신러닝 과학자들에 대한 글을 읽으면서, 어떠한 한 방법을 고집하는 것이 아닌 유용한 도구가 있으면 그것을 사용하는 것도 나쁘지 않은 방법이라는 생각이 들었고, 특히 머신러닝은 컴퓨터가 어떻게 학습하는 과정을 정확하게 이해할 수는 없지만, 우리 세계를 설명하기 위한 필수적인 도구라는 생각을 했습니다. 그리고 그러한 머신러닝은 베이커 그룹이 거리 뿐만 아니라 각도까지 더하여 학습시키는 노력이 새로운 알고리즘을 더한 알파폴드2를 따라잡을 수 없듯, 자신이 전통성을 유지하며 원래의 방법을 고수하는 것은 계속 그 자리에 머물렀다는 뜻이 될 수도 있구나 라는 것을 느끼게 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d0c0a3",
   "metadata": {},
   "source": [
    "공백 미포함 402자"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
